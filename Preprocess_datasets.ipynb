{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv(\"./datasets/tweets_training_160k.csv\", names = [\"sentiment\", \"A\", \"B\", \"C\", \"D\", \"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we are interested only in the first and the last columns, which have the sentiment, and the tweet respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = tweets.drop(tweets.columns[1:5], axis=1)\n",
    "new.to_csv('./datasets/tw_sent_160k_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    800000\n",
       "0    800000\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_tweets = pd.read_csv(\"./datasets/tw_sent_160k_train.csv\",low_memory=False,error_bad_lines=False)\n",
    "new_tweets.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The \"clean\" function cleans the tweets by removing hastags, web urls, punctuation and words that have only one letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.stem import porter\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "tok = TweetTokenizer()\n",
    "\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "added = ['.',',','-',';',':','--','\\\"','(',')', '\\'s','?','n\\'t', '<', '>',\n",
    "         '``', '\\'\\'', 'I', 'i', 'a', 'A', '..', '...', 'i\\'m', 'I\\'m']\n",
    "stop_words.extend(added)\n",
    "\n",
    "\n",
    "def clean(tweet):\n",
    "    \"\"\" Return a list of words \"\"\"\n",
    "    \n",
    "    tweet = BeautifulSoup(tweet, 'lxml').get_text()\n",
    "    \n",
    "    tweet = unicodedata.normalize('NFKD', tweet).encode('ascii', 'ignore').decode('utf-8')\n",
    "    \n",
    "\n",
    "    # clean hashtags, twitter names, web addresses, puncuation\n",
    "    tweet = (re.sub(r\"#[\\w\\d]*|@[.]?[\\w\\d]*[\\'\\w*]*|https?:\\/\\/\\S+\\b|\\\n",
    "             www\\.(\\w+\\.)+\\S*|[.,:;!?()$-/^]*\", \"\", tweet).lower())\n",
    "    \n",
    "\n",
    "    # strip repeated chars (extra vals)\n",
    "    tweet = re.sub(r\"(.)\\1\\1{1,1}\", \"\", tweet)\n",
    "    tweet = (re.sub(r\"($.)\\1{1,}\", \"\", tweet).split())\n",
    "\n",
    "    tweet = [tok.tokenize(x) for x in tweet if len(x) > 1 and x not in stop_words]\n",
    "    \n",
    "    flat_list = [item for sublist in tweet for item in sublist]\n",
    "    \n",
    "    return (\" \".join(flat_list)).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply clean to all the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snew = new_tweets[\"text\"].apply(lambda x: (clean(x)))\n",
    "new_tweets[\"text\"] = snew\n",
    "new_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By default tweets with positive sentiment have the sentiment value 4, change this to 1. So 1 for positive tweets and 0 for negative "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = new_tweets[new_tweets.sentiment == 4]\n",
    "df[\"sentiment\"] = 1\n",
    "\n",
    "\n",
    "df2 = new_tweets[new_tweets.sentiment == 0]\n",
    "\n",
    "cleaned = df2.append(df, ignore_index=True)\n",
    "cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1600000 entries, 0 to 1599999\n",
      "Data columns (total 2 columns):\n",
      "sentiment    1600000 non-null int64\n",
      "text         1600000 non-null object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 24.4+ MB\n"
     ]
    }
   ],
   "source": [
    "cleaned.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned.to_csv('./datasets/clean_sent_160k_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apple News articles dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2770 entries, 0 to 2769\n",
      "Data columns (total 2 columns):\n",
      "Date    2770 non-null int64\n",
      "News    2770 non-null object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 43.4+ KB\n"
     ]
    }
   ],
   "source": [
    "news = pd.read_csv('./datasets/news.csv')\n",
    "news.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean all the news articles text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_arr = []\n",
    "\n",
    "for x,date in zip(news['News'], news['Date']):\n",
    "    sample = x.split(\"|\")[:-1]\n",
    "    new_sam = []\n",
    "    date = str(date)\n",
    "    for sam in sample:\n",
    "        cl = clean(sam)\n",
    "        new_sam.append(cl)\n",
    "#     print(\"|\".join(new_sam))\n",
    "    new_arr.append([date[:4] + \"-\" +date[4:6] + \"-\" + date[6:], \"|\".join(new_sam)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news = pd.DataFrame(new_arr, columns=['Date', 'News'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news.to_csv('./datasets/news_clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>News</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2008-02-01</td>\n",
       "      <td>motorola weighs shift cellphones motorola said...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2008-02-04</td>\n",
       "      <td>obama mac clinton pc mac may cooler computer a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2008-02-05</td>\n",
       "      <td>499 buys apple two new products added apples l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2008-02-06</td>\n",
       "      <td>cisco profit shares fall cautious outlook netw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2008-02-07</td>\n",
       "      <td>despite rise profit cisco eases forecast netwo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date                                               News\n",
       "0  2008-02-01  motorola weighs shift cellphones motorola said...\n",
       "1  2008-02-04  obama mac clinton pc mac may cooler computer a...\n",
       "2  2008-02-05  499 buys apple two new products added apples l...\n",
       "3  2008-02-06  cisco profit shares fall cautious outlook netw...\n",
       "4  2008-02-07  despite rise profit cisco eases forecast netwo..."
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_news = pd.read_csv('./datasets/news_clean.csv')\n",
    "final_news.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apple tweets dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2008-02-01</td>\n",
       "      <td>Big, juicy, scrumptious gala apple...|OMG... T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2008-02-04</td>\n",
       "      <td>Great iPhone SummerBoard theme: Louie Mantia (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2008-02-05</td>\n",
       "      <td>Sending Problem Report for Safari to Apple! SI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2008-02-06</td>\n",
       "      <td>geez, the \"Proofreader\" in Apple Pages is terr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2008-02-07</td>\n",
       "      <td>Books for iPhone = nice web-based ebook reader...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date                                             Tweets\n",
       "0  2008-02-01  Big, juicy, scrumptious gala apple...|OMG... T...\n",
       "1  2008-02-04  Great iPhone SummerBoard theme: Louie Mantia (...\n",
       "2  2008-02-05  Sending Problem Report for Safari to Apple! SI...\n",
       "3  2008-02-06  geez, the \"Proofreader\" in Apple Pages is terr...\n",
       "4  2008-02-07  Books for iPhone = nice web-based ebook reader..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = pd.read_csv('./datasets/tweets.csv')\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the tweets about apple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aveek/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://tinyurl.com/6qt6eq\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    }
   ],
   "source": [
    "new_arr = []\n",
    "\n",
    "for x,date in zip(tweets['Tweets'], tweets['Date']):\n",
    "    sample = x.split(\"|\")\n",
    "    new_sam = []\n",
    "    date = str(date)\n",
    "    for sam in sample:\n",
    "        cl = clean(sam)\n",
    "        new_sam.append(cl)\n",
    "#     print(\"|\".join(new_sam))\n",
    "    new_arr.append([date, \"|\".join(new_sam)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets = pd.DataFrame(new_arr, columns=['Date', 'Tweets'])\n",
    "df_tweets.to_csv('datasets/tweets_clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
